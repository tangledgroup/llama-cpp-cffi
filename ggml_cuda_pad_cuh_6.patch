--- llama.cpp-master/ggml/src/ggml-cuda/pad.cuh	2024-12-04 20:47:44.575406091 +0100
+++ llama.cpp/ggml/src/ggml-cuda/pad.cuh	2024-12-05 10:54:53.641599893 +0100
@@ -3,3 +3,4 @@
 #define CUDA_PAD_BLOCK_SIZE 256
 
 void ggml_cuda_op_pad(ggml_backend_cuda_context & ctx, ggml_tensor * dst);
+void ggml_cuda_op_unpad(ggml_backend_cuda_context & ctx, ggml_tensor * dst);
