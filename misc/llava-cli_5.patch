--- llama.cpp-master/examples/llava/llava-cli.cpp	2024-11-25 14:16:56.886772502 +0100
+++ llama.cpp/examples/llava/llava-cli.cpp	2024-11-26 11:40:24.162538867 +0100
@@ -13,6 +13,20 @@
 #include <cstring>
 #include <vector>
 
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#ifdef LLAMA_LIB
+typedef void (*_llama_yield_token_t)(const char * token);
+typedef int (*_llama_should_stop_t)(void);
+int _llava_cli_main(int argc, char ** argv, _llama_yield_token_t _llama_yield_token, _llama_should_stop_t _llama_should_stop);
+#endif
+
+#ifdef __cplusplus
+}
+#endif
+
 static bool eval_tokens(struct llama_context * ctx_llama, std::vector<llama_token> tokens, int n_batch, int * n_past) {
     int N = (int) tokens.size();
     for (int i = 0; i < N; i += n_batch) {
@@ -146,7 +160,11 @@
     return embed;
 }
 
+#ifdef LLAMA_LIB
+static void process_prompt(struct llava_context * ctx_llava, struct llava_image_embed * image_embed, common_params * params, const std::string & prompt, _llama_yield_token_t _llama_yield_token, _llama_should_stop_t _llama_should_stop) {
+#else
 static void process_prompt(struct llava_context * ctx_llava, struct llava_image_embed * image_embed, common_params * params, const std::string & prompt) {
+#endif
     int n_past = 0;
 
     const int max_tgt_len = params->n_predict < 0 ? 256 : params->n_predict;
@@ -203,7 +221,21 @@
         response += tmp;
         if (strcmp(tmp, "</s>") == 0) break;
         if (strstr(tmp, "###")) break; // Yi-VL behavior
+
+#ifdef LLAMA_LIB
+        if (_llama_yield_token != NULL) {
+            _llama_yield_token(tmp);
+        }
+
+        if (_llama_should_stop != NULL) {
+            if (_llama_should_stop() == 1) {
+                break;
+            }
+        }
+#else
         LOG("%s", tmp);
+#endif
+
         if (strstr(response.c_str(), "<|im_end|>")) break; // Yi-34B llava-1.6 - for some reason those decode not as the correct token (tokenizer works)
         if (strstr(response.c_str(), "<|im_start|>")) break; // Yi-34B llava-1.6
         if (strstr(response.c_str(), "USER:")) break; // mistral llava-1.6
@@ -269,7 +301,15 @@
     llama_backend_free();
 }
 
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#ifdef LLAMA_LIB
+int _llava_cli_main(int argc, char ** argv, _llama_yield_token_t _llama_yield_token, _llama_should_stop_t _llama_should_stop) {
+#else
 int main(int argc, char ** argv) {
+#endif
     ggml_time_init();
 
     common_params params;
@@ -297,7 +337,11 @@
         auto * image_embed = load_image(ctx_llava, &params, "");
 
         // process the prompt
+#ifdef LLAMA_LIB
+        process_prompt(ctx_llava, image_embed, &params, params.prompt, _llama_yield_token, _llama_should_stop);
+#else
         process_prompt(ctx_llava, image_embed, &params, params.prompt);
+#endif
 
         llama_perf_context_print(ctx_llava->ctx_llama);
         llava_image_embed_free(image_embed);
@@ -314,7 +358,11 @@
             }
 
             // process the prompt
+#ifdef LLAMA_LIB
+            process_prompt(ctx_llava, image_embed, &params, params.prompt, _llama_yield_token, _llama_should_stop);
+#else
             process_prompt(ctx_llava, image_embed, &params, params.prompt);
+#endif
 
             llama_perf_context_print(ctx_llava->ctx_llama);
             llava_image_embed_free(image_embed);
@@ -327,3 +375,7 @@
 
     return 0;
 }
+
+#ifdef __cplusplus
+}
+#endif
